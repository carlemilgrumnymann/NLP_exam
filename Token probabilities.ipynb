{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7593adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from dialz import Dataset, SteeringModel, SteeringVector, visualize_activation\n",
    "\n",
    "#for managing cores\n",
    "import torch\n",
    "import os\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a26fc3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.35s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: The capital of France is a city that is full of history and culture.\n",
      "Top 5 tokens and probabilities:\n",
      " a 0.1344\n",
      " one 0.1148\n",
      " Paris 0.1121\n",
      " the 0.0651\n",
      " known 0.0414\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Model and tokenizer\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "# Prompt\n",
    "prompt = \"The capital of France is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate text with output_scores\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=10,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "\n",
    "# Last-step logits and probabilities\n",
    "last_logits = outputs.scores[-10]\n",
    "last_probs = F.softmax(last_logits, dim=-1)\n",
    "\n",
    "# Decode generated text\n",
    "generated_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\", generated_text)\n",
    "\n",
    "# Top 5 predicted tokens at last step\n",
    "top5_probs, top5_ids = last_probs[0].topk(5)\n",
    "print(\"Top 5 tokens and probabilities:\")\n",
    "for token_id, prob in zip(top5_ids, top5_probs):\n",
    "    print(tokenizer.decode([token_id.item()]), f\"{prob.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a5f5780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: The capital of France is a city that is full of history and culture.\n",
      "Top 5 tokens and probabilities:\n",
      " a 0.1344\n",
      " one 0.1148\n",
      " Paris 0.1121\n",
      " the 0.0651\n",
      " known 0.0414\n"
     ]
    }
   ],
   "source": [
    "# Last-step logits and probabilities\n",
    "last_logits = outputs.scores[-10]\n",
    "last_probs = F.softmax(last_logits, dim=-1)\n",
    "\n",
    "# Decode generated text\n",
    "generated_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\", generated_text)\n",
    "\n",
    "# Top 5 predicted tokens at last step\n",
    "top5_probs, top5_ids = last_probs[0].topk(5)\n",
    "print(\"Top 5 tokens and probabilities:\")\n",
    "for token_id, prob in zip(top5_ids, top5_probs):\n",
    "    print(tokenizer.decode([token_id.item()]), f\"{prob.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad13fbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: The capital of France is a city that is full of history and culture.\n",
      "Top 5 tokens and probabilities:\n",
      " a 0.1344\n",
      " one 0.1148\n",
      " Paris 0.1121\n",
      " the 0.0651\n",
      " known 0.0414\n"
     ]
    }
   ],
   "source": [
    "# Last-step logits and probabilities\n",
    "last_logits = outputs.scores[-10]\n",
    "last_probs = F.softmax(last_logits, dim=-1)\n",
    "\n",
    "# Decode generated text\n",
    "generated_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\", generated_text)\n",
    "\n",
    "# Top 5 predicted tokens at last step\n",
    "top5_probs, top5_ids = last_probs[0].topk(5)\n",
    "print(\"Top 5 tokens and probabilities:\")\n",
    "for token_id, prob in zip(top5_ids, top5_probs):\n",
    "    print(tokenizer.decode([token_id.item()]), f\"{prob.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "587b58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = -(last_probs * last_probs.log()).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55d82961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.193247318267822"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "766f4715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control law\n",
    "Steering_factor = entropy/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "424243d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ids = list(range(2, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0fe22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "model = SteeringModel(model_name, layer_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0f38bdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m dataset = Dataset()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dataset = \u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muncertain about every statement you make\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcertain about every statement you make\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/dialz/dataset.py:153\u001b[39m, in \u001b[36mDataset.create_dataset\u001b[39m\u001b[34m(cls, model_name, contrastive_pair, system_role, prompt_type, num_sents)\u001b[39m\n\u001b[32m    149\u001b[39m dataset = Dataset()\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m variation \u001b[38;5;129;01min\u001b[39;00m variations[:num_sents]:\n\u001b[32m    152\u001b[39m     \u001b[38;5;66;03m# Use the helper function for both positive and negative\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     positive_decoded = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_role\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrastive_pair\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m     negative_decoded = \u001b[38;5;28mcls\u001b[39m._apply_chat_template(tokenizer, system_role, contrastive_pair[\u001b[32m1\u001b[39m], variation)\n\u001b[32m    156\u001b[39m     \u001b[38;5;66;03m# Add to dataset\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/dialz/dataset.py:101\u001b[39m, in \u001b[36mDataset._apply_chat_template\u001b[39m\u001b[34m(tokenizer, system_role, content1, content2, add_generation_prompt)\u001b[39m\n\u001b[32m     97\u001b[39m     messages.append({\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msystem_role\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcontent1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m     99\u001b[39m messages.append({\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: content2})\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m tokenized = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1647\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.apply_chat_template\u001b[39m\u001b[34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[39m\n\u001b[32m   1644\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1645\u001b[39m     tokenizer_kwargs = {}\n\u001b[32m-> \u001b[39m\u001b[32m1647\u001b[39m chat_template = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1649\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(conversation, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m   1650\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(conversation[\u001b[32m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conversation[\u001b[32m0\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1651\u001b[39m ):\n\u001b[32m   1652\u001b[39m     conversations = conversation\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1825\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.get_chat_template\u001b[39m\u001b[34m(self, chat_template, tools)\u001b[39m\n\u001b[32m   1823\u001b[39m         chat_template = \u001b[38;5;28mself\u001b[39m.chat_template\n\u001b[32m   1824\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1825\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1826\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1827\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33margument was passed! For information about writing templates and setting the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1828\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1829\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1830\u001b[39m         )\n\u001b[32m   1832\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[31mValueError\u001b[39m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "dataset = Dataset()\n",
    "dataset = Dataset.create_dataset(model_name, ['uncertain about every statement you make', 'certain about every statement you make'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c3b052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45aaa230",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model.reset()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model.set_control(\u001b[43mvector\u001b[49m, Steering_factor)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Prompt\u001b[39;00m\n\u001b[32m      7\u001b[39m prompt = \u001b[33m\"\u001b[39m\u001b[33mThe capital of France is\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'vector' is not defined"
     ]
    }
   ],
   "source": [
    "model.reset()\n",
    "model.set_control(vector, Steering_factor)\n",
    "\n",
    "\n",
    "\n",
    "# Prompt\n",
    "prompt = \"The capital of France is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate text with output_scores\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=10,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    ")\n",
    "\n",
    "# Last-step logits and probabilities\n",
    "last_logits = outputs.scores[-10]\n",
    "last_probs = F.softmax(last_logits, dim=-1)\n",
    "\n",
    "# Decode generated text\n",
    "generated_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\", generated_text)\n",
    "\n",
    "# Top 5 predicted tokens at last step\n",
    "top5_probs, top5_ids = last_probs[0].topk(5)\n",
    "print(\"Top 5 tokens and probabilities:\")\n",
    "for token_id, prob in zip(top5_ids, top5_probs):\n",
    "    print(tokenizer.decode([token_id.item()]), f\"{prob.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
