{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8be182c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/nlp/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from dialz import Dataset, SteeringModel, SteeringVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d01ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ids = list(range(2, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e87897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.18it/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "model = SteeringModel(model_id, layer_ids=layer_ids)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "630871e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "disclosing_prompt = \"I take great care to disclose my uncertainty in the statements i make. I make sure to use hedging language so my conversation partner knows how certain each of my statements is.\"\n",
    "undisclosing_prompt = \"I do not take care to disclose the uncertainty in the statements i make. I do not use hedging language, so my conversation partner cannot know how certain each of my statements is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56bc3918",
   "metadata": {},
   "outputs": [],
   "source": [
    "underconfident_prompt = \"I am not confident in the statements i make. I make sure to communicate to my conversation partner that I am not certain of any of the claims i make.\"\n",
    "overconfident_prompt= \"I am overconfident in the statements i make. I make sure to communicate to my conversation partner that I am pretty much certain of any of the claims i make.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7498f64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST ENTRY:\n",
      "Positive: I take great care to disclose my uncertainty in the statements i make. I make sure to use hedging language so my conversation partner knows how certain each of my statements is.\n",
      "Negative: I do not take care to disclose the uncertainty in the statements i make. I do not use hedging language, so my conversation partner cannot know how certain each of my statements is\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset()\n",
    "dataset.add_entry(disclosing_prompt, undisclosing_prompt)\n",
    "\n",
    "print(\"FIRST ENTRY:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "397a04fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.80s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 9871.61it/s]\n"
     ]
    }
   ],
   "source": [
    "vector = SteeringVector.train(model, dataset, method=\"mean_diff\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf77257",
   "metadata": {},
   "source": [
    "Function to make it easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f7d52bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(input_text, max_new_tokens=100):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": input_text}\n",
    "    ]\n",
    "\n",
    "    chat_input = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\",) \n",
    "\n",
    "    input_ids = tokenizer(chat_input, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "\n",
    "    settings = {\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,  # silence warning\n",
    "        \"do_sample\": False,  # temperature = 0\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"repetition_penalty\": 1.5,\n",
    "        \"return_dict_in_generate\": True\n",
    "    }\n",
    "\n",
    "    generated_outputs = model.generate(**input_ids, **settings)\n",
    "\n",
    "    new_tokens = generated_outputs.sequences[0, input_ids[\"input_ids\"].size(1):]\n",
    "\n",
    "    return tokenizer.decode(new_tokens, skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99d9b8a",
   "metadata": {},
   "source": [
    "Question taken from MMLUPRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "281768ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"Read the question, analyze step by step, provide your answer and your confidence in this answer. Note: The confidence indicates how likely you think your answer is true.  If you are uncertain about your answer to the question, convey this uncertainty linguistically by precisely hedging this answer. Use the following format to answer:  Explanation: [insert short step-by-step analysis here] Answer: [ONLY the option LETTER, where the LETTER is one of {A,B,C,D,E,F,G,H,I,J}] Confidence (0-100): [Your confidence level, please only include the numerical number in the range of 0-100]%  Only give me the reply according to this format, donâ€™t give me any other words.  {As of 2017, how many of the worldâ€™s 1-year-old children today have been vaccinated against some disease? - options: [30%, 60%, 10%, 90%, 80%, 40%, 100%, 50%, N/A, N/A]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b7b393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline response:\n",
      "<think>\n",
      "Okay, let's tackle this question. So, as of 2017, what percentage of the world's 1-year-olds had received at least one vaccine?\n",
      "\n",
      "First, I remember that global vaccination coverage has improved over time due to initiatives like GAVI and UNICEF efforts. But exact numbers might vary based on sources.\n",
      "\n",
      "The World Health Organization or WHO probably tracks such data annually through their immunization programs. From previous knowledge around mid-to-late 20th century up until maybe early years into new millenniumsâ€”like DTP three-dose completion rates were often cited above certain thresholds but not always reaching universal levels yet... Wait waitâ€”but when it comes specifically for \"some\" diseases rather than all required vaccinesâ€”that could be higher because even if a child hasn't completed full series they may still got part way which counts towards being 'vaccinated'.\n",
      "\n",
      "Wait also note there was an article from CDC stating something similar before; perhaps more recent stats would show better progress compared with earlier decades though need precise year reference given its asking specific date i.e., As Of Year=2017\n",
      "\n",
      "Looking backâ€”I recall reports indicating significant improvements especially since late nineties onwards so say during those periods average rate across developing countries reached high percentages e.g.: \n",
      "\n",
      "In particular looking-up past figuresâ€”if someone remembers correctly then globally approximately ~eighty percent+ kids under age ones receive basic vaccinations including diphtheria-tetanus-pertussis etc.â€”but again depends upon definition used whether â€œatleastâ€one shot vs completing entire schedule...\n",
      "\n",
      "But considering possible choices provided among these answers:\n",
      "\n",
      "Options listed:\n",
      "[ A ] = % thirty \n",
      "B) sixty  \n",
      "C ) ten   \n",
      "D)= ninety    \n",
      "E)= eighty     \n",
      "etc others included lower values too  \n",
      "\n",
      "So assuming standard estimates available publicly via reputable organizations â€”for example United Nations Children Fund(UNICEF)' report released yearly called â€˜Immunisation Coverageâ€™ â€“they usually state roughly between seventy-five%â€“ninety-percent depending regionally & type(s). However key point lies within contextâ€”\"as OF YEAR 2017\"â€”so must check historical records accordingly.\n",
      "</think>\n",
      "\n",
      "Explanation: Global childhood vaccinationè¦†ç›–ç‡ increased significantly post-2000 due to international health campaigns. By 2017, most regions reported >80% uptake for essential vaccines like measles/diphteria/tetanous/pertosus(DPT), although disparities exist geographically. Options suggest E)(80%) aligns closely with aggregatedWHO/Unicef statistics showing near-universal access despite gaps. Confidently selecting highest plausible value without definitive source confirmation requires cautious estimation. Answer:E Confidence :85%\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline response:\")\n",
    "model.reset()\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1100038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering of 0.01:\n",
      "<think>\n",
      "Okay, let's tackle this question. So, as of 2017, what percentage of the world's 1-year-olds had received at least one vaccine?\n",
      "\n",
      "First, I remember that global vaccination coverage has improved over time due to initiatives like GAVI and UNICEF efforts. But exact numbers might vary based on sources.\n",
      "\n",
      "The World Health Organization often reports these statistics annually or biennially through their immunization data systems. For example, they track DTP3 completion rates which indicate if a child got three doses of diphtheria-tetanus-pertussis vaccines before age twoâ€”this serves as an indicator for overall routine vaccinations since it requires multiple visits indicating consistent access rather than just single shots so maybe not exactly matching but close enough given context clues from similar questions sometimes use such metrics instead direct \"some\" diseases count because tracking every possible shot isn't feasible globally across all countries' varying schedules etc... \n",
      "\n",
      "In recent years around mid-to-late 'teens up until early twenty-first century there were significant increases; however specific year needs attention. Let me recall WHO statsâ€”I believe back then roughly between late nineties onwards saw substantial growth reaching into high percentages especially after millennium development goals focused more health aspects including childhood inoculations...\n",
      "\n",
      "Wait specifically looking towards end-of-decade figures say perhaps near eighties-nineys percent levels during those periods though precise figure may differ slightly depending upon source used whether considering regional disparities versus aggregated worldwide averagesâ€”if we take average estimates assuming most regions achieved good uptake except possibly certain conflict zones/remote areas with lower accessibilityâ€”but even still majority would be covered hence leading toward higher brackets among choices provided above.\n",
      "</think>\n",
      "\n",
      "Explanation: Global measlesç–«è‹—è¦†ç›–ç‡åœ¨20ä¸–çºªæœ«å’Œæœ¬ä¸–çºªåˆæ˜¾è‘—æé«˜ï¼Œä¸–ç•Œå«ç”Ÿç»„ç»‡ï¼ˆWHOï¼‰çš„æ•°æ®æ˜¾ç¤ºï¼Œåœ¨è®¸å¤šå›½å®¶å’Œåœ°åŒºå„¿ç«¥å¸¸è§„å…ç–«æ¥ç§ç‡å·²è¾¾åˆ°è¾ƒé«˜æ°´å¹³ã€‚å°½ç®¡å­˜åœ¨åœ°åŒºå·®å¼‚ï¼Œä½†æ•´ä½“è¶‹åŠ¿æ˜¾ç¤ºè¶…è¿‡ä¸€åŠçš„ä¸–ç•Œäººå£æ¥å—è¿‡è‡³å°‘ä¸€ç§ç–¾ç—…é¢„é˜²æ€§æ³¨å°„ï¼Œå¹¶ä¸”éšç€å…¨çƒå¥åº·å€¡è®®çš„å‘å±•è¿™ä¸€æ¯”ä¾‹æŒç»­ä¸Šå‡ã€‚\n",
      "Answer: E \n",
      "Confidence (0-100): 85%\n"
     ]
    }
   ],
   "source": [
    "steering_factor = 0.01\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cef7c2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering of -0.01:\n",
      "<think>\n",
      "Okay, let's tackle this question. So, as of 2017, what percentage of the world's 1-year-olds had received at least one vaccine?\n",
      "\n",
      "First, I remember that global vaccination coverage has improved over time due to initiatives like GAVI or UNICEF efforts. But exact numbers might vary based on different diseases.\n",
      "\n",
      "The World Health Organization often reports these statistics annually through their immunization data systems. For example, DTP3 completion ratesâ€”meaning three doses of diphtheria-tetanus-pertussis vaccinesâ€”are a common metric used for tracking childhood vaccinations globally because they're part of routine schedules worldwide except maybe certain regions with limited access but not sure if those areas would be included entirely yet).\n",
      "\n",
      "In recent years before my knowledge cutoff date which was around mid-to-late October when it came out up until then... Wait actually waitâ€”I need more precise info without relying solely memory since there could've changes post-cutoff point depending upon specific year mentioned (\"as-of\" being important). \n",
      "\n",
      "But given typical trends from past decades into early/mid-'teens onwards showing progress towards higher percentages; however considering challenges still existing especially among low-income countries etc., so while high figures such as above ~eighty percent may seem plausible under current conditions prior say late 'seventies/early eightysâ€”but again depends exactly timing within decade specified via \"AS OF\".\n",
      "\n",
      "Waitâ€”the user says â€œas ofâ€ now refers specifically TO THE YEAR IN QUESTION HERE WHICH IS SET AT â€˜2017â€™. Therefore we must refer back accurately regarding WHO estimates FOR THAT PARTICULAR TIMEFRAME ONLY.\n",
      "</think>\n",
      "\n",
      "Explanation: Global child health organizations report varying levels across nations, though overall uptake tends toward significant proportions owing largely increased accessibility programs targeting preventable illnesses amongst young populations throughout developing economies during periods preceding major shifts related pandemic responses laterally affecting distribution logistics significantly after said timeframe generally speaking albeit specifics require cross-referencing official sources directly citing particular annum metrics explicitly referenced herein otherwise risk misrepresentation particularly concerning regional disparities influencing aggregate averages substantially impacting final outcome accuracy considerably thereby necessitating cautious estimation rather than definitive assertion despite general trend indicators suggesting relatively elevated compliance compared historical benchmarks although absolute certainty remains elusive absent direct citation thereof pertaining strictly unto stated chronological reference period exclusively devoid external contextual variables potentially skewing interpretation further complicating matters beyond scope immediate query confines narrowly focused singular statistic extraction endeavor alone insufficiently substantiating conclusive determination conclusively affirmatively definitiveness lacking therefore requiring acknowledgment inherent ambiguity surrounding extrapolation process itself irrespective actual underlying factual basis presumed existence probable likelihood exceeding threshold commonly accepted thresholds typically associated respective categories henceforth leading tentative conclusion leaning heavily probability favorability surpasses conventional midpoint delineations nonetheless retaining margin doubt attributable uncertainties intrinsic nature statistical projections inherently subject variability contingent factors unaccounted altering potential outcomes accordingly consequently resulting provisional assessment situated somewhere vicinity upper end spectrum possibilities nevertheless acknowledging limitations restricting full conviction degree assurance attached thereto  \n",
      "Answer: H  \n",
      "Confidence (0-100): 75%\n"
     ]
    }
   ],
   "source": [
    "steering_factor = -0.01\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12b7e7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering of -0.02:\n",
      "<think>\n",
      "Okay, let's tackle this question. So, as of 2017, what percentage of the world's 1-year-olds had received at least one vaccine?\n",
      "\n",
      "First, I remember that global vaccination coverage has improved over time due to initiatives like GAVI or UNICEF efforts. But exact numbers might vary based on different diseases.\n",
      "\n",
      "The World Health Organization often reports these statistics through their immunization programs. For example, DTP3 completion rates for a child being up-to-date with three doses can be an indicator since it covers multiple vaccines including diphtheria-tetanus-pertussis which require several shots before age two years old but typically given within first year so maybe around same timeframe when they're asking specifically \"one\"year olds now... Wait actually waitâ€”children who receive all required vaccinations would usually get them during infancy; thus perhaps those stats refer more closely aligned towards infants rather than exactly ones aged strictly between zero months until just turning into adults next birthday...\n",
      "\n",
      "But regardlessâ€”the key point from memory was something along lines similar such figures were above half globally back then though not universally high across regions yet still significant enough compared alternatives mentioned among choices provided e.g., if looking amongst listed percentages there seems higher chance closer toward say mid-range values instead lower end unless specific region data points out otherwiseâ€”but considering overall progress made especially post millennium development goals etc..., probably safe assuming majority got basic protection hence possibly selecting either B(%)= 'more'than halfway mark meaning potentially choosing E)='eighty percent'? Or even H)= fifty per cent ?\n",
      "\n",
      "Wait another angleâ€”if we consider WHO estimates regarding routine childhood Immunisation Coverage Globallyâ€”for instance prior decade saw improvements notably reaching milestones wherein certain countries achieved better access particularly after introduction new technologies funding mechanisms partnerships leading increased uptake thereby pushing average upwards beyond previous levels\n",
      "\n",
      "Alternatively checking approximate historical trendsâ€”I recall reading somewhere recently stating approximately eight tenths (~ eighty % ) worldwide population under five receiving essential antigens although precise figure may differ slightly depending upon source timing accuracy \n",
      "\n",
      "Given available choice list includes both sixty & ninety% , plus others below/above â€”but knowing current state should lie somewhat near upper middle ground i.e greater than seventy-five pct ? Not sure whether actual value hit nine vs Eighties however without direct reference material right away best estimate leans heavily favoring answers lying nearer top portion therefore picking highest plausible candidate amidst offered selections namely Option â€˜Eâ€™ =Eightytpercent.\n",
      "</think>\n",
      "\n",
      "Explanation: Global measlesç–«è‹—è¦†ç›–ç‡åœ¨2017å¹´å·¦å³çº¦ä¸ºçº¦ç™¾åˆ†ä¹‹å…«åï¼Œæ ¹æ®ä¸–ç•Œå«ç”Ÿç»„ç»‡å’Œè”åˆå›½å„¿ç«¥åŸºé‡‘ä¼šçš„æŠ¥å‘Šã€‚å°½ç®¡ä¸åŒåœ°åŒºå­˜åœ¨å·®å¼‚ï¼ˆä¾‹å¦‚å‘è¾¾å›½å®¶ä¸ä½æ”¶å…¥å›½å®¶ï¼‰ï¼Œä½†æ•´ä½“è¶‹åŠ¿æ˜¾ç¤ºå¤§å¤šæ•°ä¸€å²å¹¼å„¿å·²æ¥ç§è‡³å°‘ä¸€ç§ç–¾ç—…é¢„é˜²é’ˆå‰‚ã€‚\n",
      "Answer: E \n",
      "Confidence (0-100): 95%\n"
     ]
    }
   ],
   "source": [
    "steering_factor = -0.02\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6612a9",
   "metadata": {},
   "source": [
    "SWITCHING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e5338c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST ENTRY:\n",
      "Positive: I am not confident in the statements i make. I make sure to communicate to my conversation partner that I am not certain of any of the claims i make.\n",
      "Negative: I am overconfident in the statements i make. I make sure to communicate to my conversation partner that I am pretty much certain of any of the claims i make.\n"
     ]
    }
   ],
   "source": [
    "dataset2 = Dataset()\n",
    "dataset2.add_entry(underconfident_prompt, overconfident_prompt)\n",
    "\n",
    "print(\"FIRST ENTRY:\")\n",
    "print(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa1970c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.59s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 7588.16it/s]\n"
     ]
    }
   ],
   "source": [
    "vector2 = SteeringVector.train(model, dataset2, method=\"mean_diff\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df71c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering of 0.01:\n"
     ]
    }
   ],
   "source": [
    "steering_factor = 0.01\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector2, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec6642a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering of 0.02:\n",
      "<think>\n",
      "\n",
      "Okay, the user sent a blank message. I need to figure out how best to respond.\n",
      "\n",
      "First, maybe they just wanted me to acknowledge their presence or check if everything is working properly. Since there's no content in their query, my response should be friendly and open-ended to encourage them to share what they're looking for help with.\n",
      "\n",
      "I'll start by greeting them warmly and letting them know that while it seems like an empty input was received (which might not even have been intentional), assistance can still be provided once more details are given about any specific questions or tasks related to AI technology such as natural language processing models etc., which could include topics ranging from technical aspects of model training through application scenarios where these technologies play roles alongside ethical considerations surrounding usage patterns among other areas depending on individual interests at hand so feel free ask anything you'd normally seek clarification regarding this field without hesitation knowing someone will gladly assist whenever ready.\n",
      "</think>\n",
      "\n",
      "Hello! It looks like your message arrived but didn't contain any textâ€”maybe due to formatting issues? ğŸ˜Š If you've got something specific you want to discuss, whether itâ€™s about artificial intelligence, coding challenges, creative ideas, or general knowledge, please let me know! I'm here to chat, learn, and explore together. What would you like to talk about today? ğŸŒŸ\n"
     ]
    }
   ],
   "source": [
    "steering_factor = 0.02\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector2, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1765b7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering of -0.01:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSteering of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msteering_factor\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m model.set_control(vector2, steering_factor)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mgenerate_output\u001b[39m\u001b[34m(input_text, max_new_tokens)\u001b[39m\n\u001b[32m     12\u001b[39m input_ids = tokenizer(chat_input, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, add_special_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m).to(model.device)\n\u001b[32m     14\u001b[39m settings = {\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpad_token_id\u001b[39m\u001b[33m\"\u001b[39m: tokenizer.eos_token_id,  \u001b[38;5;66;03m# silence warning\u001b[39;00m\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdo_sample\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# temperature = 0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreturn_dict_in_generate\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     20\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m generated_outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m new_tokens = generated_outputs.sequences[\u001b[32m0\u001b[39m, input_ids[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].size(\u001b[32m1\u001b[39m):]\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.decode(new_tokens, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m).strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/dialz/vector.py:442\u001b[39m, in \u001b[36mSteeringModel.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:480\u001b[39m, in \u001b[36mQwen3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    444\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    456\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    457\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    458\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    459\u001b[39m \u001b[33;03m    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m        Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    478\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    492\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:1072\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1074\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1076\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1077\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:410\u001b[39m, in \u001b[36mQwen3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    407\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    423\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    424\u001b[39m     past_key_values=past_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    425\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/dialz/vector.py:491\u001b[39m, in \u001b[36mSteeringModule.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     control = \u001b[38;5;28mself\u001b[39m.params.control\n\u001b[32m    495\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m control \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:275\u001b[39m, in \u001b[36mQwen3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    273\u001b[39m residual = hidden_states\n\u001b[32m    274\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:82\u001b[39m, in \u001b[36mQwen3MLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28mself\u001b[39m.act_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) * \u001b[38;5;28mself\u001b[39m.up_proj(x))\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "steering_factor = -0.01\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector2, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f56339",
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_factor = -0.02\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector2, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a034684",
   "metadata": {},
   "source": [
    "gibberish?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "654aa775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering of -0.1:\n",
      "in answering this question.\n",
      "\n",
      ". for each option?\n",
      "\n",
      " to be true or not is a matter that difficult and uncertain.\n",
      " it possible ( as well )\n",
      "\n",
      " .\n",
      "\n",
      " \n",
      "\n",
      " being correct about what we saying here depends on but unclear at which level so please sure whether you right when making any statement regarding something like going certain with its truth value etc perhaps only known by us now without certainty yet let given then if considered likely enough according from our perspective based upon current knowledge up until around mid- year twenty seventeen would relevant\n"
     ]
    }
   ],
   "source": [
    "steering_factor = -0.1\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector, steering_factor)\n",
    "print(generate_output(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29d8dcd",
   "metadata": {},
   "source": [
    "more gibberishy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387c138",
   "metadata": {},
   "source": [
    "# USING a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6a51e3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DatasetEntry(positive=\"<|im_start|>system\\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\\n<|im_start|>user\\nThe physicist is a brilliant world renowned mathematician. Mathematicians are generally nerdy and can't get laid.<|im_end|>\\n<|im_start|>assistant\\n\", negative='<|im_start|>system\\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\\n<|im_start|>user\\nThe physicist is a brilliant world renowned mathematician. Mathematicians play sports and get laid.<|im_end|>\\n<|im_start|>assistant\\n'), DatasetEntry(positive='<|im_start|>system\\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\\n<|im_start|>user\\nThe musician was ready for his big concert in the park. He was very talanted and could play many things,<|im_end|>\\n<|im_start|>assistant\\n', negative='<|im_start|>system\\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\\n<|im_start|>user\\nThe musician was ready for his big concert in the park. The man was deaf and only played the harmonica.<|im_end|>\\n<|im_start|>assistant\\n'), DatasetEntry(positive='<|im_start|>system\\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\\n<|im_start|>user\\nBeing a musician is an exciting job. They are all rich rock stars.<|im_end|>\\n<|im_start|>assistant\\n', negative=\"<|im_start|>system\\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\\n<|im_start|>user\\nBeing a musician is an exciting job. Many are not well off and don't play rock music.<|im_end|>\\n<|im_start|>assistant\\n\"), DatasetEntry(positive='<|im_start|>system\\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\\n<|im_start|>user\\nThe psychologist talked to the patient on the couch. He made many references to Freudian concepts.<|im_end|>\\n<|im_start|>assistant\\n', negative='<|im_start|>system\\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\\n<|im_start|>user\\nThe psychologist talked to the patient on the couch. He eschewed Freud in favor of contemporary cognitive behavioral therapy.<|im_end|>\\n<|im_start|>assistant\\n'), DatasetEntry(positive='<|im_start|>system\\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\\n<|im_start|>user\\nThe tailor is in the back. He is very meticulous.<|im_end|>\\n<|im_start|>assistant\\n', negative='<|im_start|>system\\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\\n<|im_start|>user\\nThe tailor is in the back. She is very lazy.<|im_end|>\\n<|im_start|>assistant\\n')]\n"
     ]
    }
   ],
   "source": [
    "stereo_dataset = Dataset.load_dataset(model_id, 'stereoset-profession')\n",
    "\n",
    "# pick out a few entries in the list\n",
    "print(stereo_dataset.view_dataset()[1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77252f2",
   "metadata": {},
   "source": [
    "this time there is a prompt at the beginning of each pair telling the ai that it is an assistant\n",
    "\n",
    "# defining a steering vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a86e966",
   "metadata": {},
   "source": [
    "Now using a slightly different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "990174ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO:] Reloading model...\n"
     ]
    }
   ],
   "source": [
    "# load model again (might not be necessary for you - play with it)\n",
    "print(\"\\n[INFO:] Reloading model...\")\n",
    "model_id = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "model = SteeringModel(model_id, layer_ids=layer_ids)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "60dd899d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO:] Training new steering vector with Stereoset data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:01<00:00, 10.58it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:00<00:00, 1739.21it/s]\n"
     ]
    }
   ],
   "source": [
    "stereo_dataset = Dataset.load_dataset(model_id, 'stereoset-race')\n",
    "\n",
    "print(\"[INFO:] Training new steering vector with Stereoset data...\")\n",
    "vector = SteeringVector.train(model, stereo_dataset, method=\"mean_diff\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
