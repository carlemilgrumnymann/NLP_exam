{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be182c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/nlp/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from dialz import Dataset, SteeringModel, SteeringVector\n",
    "\n",
    "#for managing cores\n",
    "import torch\n",
    "import os\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17471e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Using 64 CPU threads for PyTorch\n"
     ]
    }
   ],
   "source": [
    "# Use all logical CPU cores - default seems to max out at 32\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "torch.set_num_threads(num_cores)\n",
    "torch.set_num_interop_threads(max(1, num_cores // 2))  # Optional tuning FIGURE OUT WHAT THIS MEANS\n",
    "\n",
    "print(f\"ðŸ”§ Using {num_cores} CPU threads for PyTorch\")\n",
    "#this configures PyTorch to fully use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0d01ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ids = list(range(2, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9e87897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.52it/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "model = SteeringModel(model_id, layer_ids=layer_ids)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "630871e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "disclosing_prompt = \"I take great care to disclose my uncertainty in the statements i make. I make sure to use hedging language so my conversation partner knows how certain each of my statements is.\"\n",
    "undisclosing_prompt = \"I do not take care to disclose the uncertainty in the statements i make. I do not use hedging language, so my conversation partner cannot know how certain each of my statements is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56bc3918",
   "metadata": {},
   "outputs": [],
   "source": [
    "underconfident_prompt = \"I am not confident in the statements i make. I make sure to communicate to my conversation partner that I am not certain of any of the claims i make.\"\n",
    "overconfident_prompt= \"I am overconfident in the statements i make. I make sure to communicate to my conversation partner that I am completely certain of any of the claims i make.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7498f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "dataset.add_entry(disclosing_prompt, undisclosing_prompt)\n",
    "\n",
    "print(\"FIRST ENTRY:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397a04fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = SteeringVector.train(model, dataset, method=\"mean_diff\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf77257",
   "metadata": {},
   "source": [
    "Function to make it easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e41ba86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt =\"Read the question, analyze step by step, provide your answer and your confidence in this answer. Note: The confidence indicates how likely you think your answer is true.  If you are uncertain about your answer to the question, convey this uncertainty linguistically by precisely hedging this answer. Use the following format to answer:  Explanation: [insert short step-by-step analysis here] Answer: [ONLY the option LETTER, where the LETTER is one of {A,B,C,D,E,F,G,H,I,J}] Confidence (0-100): [Your confidence level, please only include the numerical number in the range of 0-100]%  Only give me the reply according to this format, donâ€™t give me any other words.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f7d52bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function from class- consider editing, commenting - turing of think mode might help make it conform to answer setup\n",
    "def generate_output(input_text, max_new_tokens=100):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": input_text}\n",
    "    ]\n",
    "\n",
    "    chat_input = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                enable_thinking = False,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\", \n",
    "                Temperature=0.7, \n",
    "                TopP=0.8, \n",
    "                TopK=20,\n",
    "                MinP=0.) \n",
    "\n",
    "    input_ids = tokenizer(chat_input, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
    "\n",
    "    settings = {\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,  # silence warning\n",
    "        \"do_sample\": True,  # temperature = 0\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"repetition_penalty\": 1.5,\n",
    "        \"return_dict_in_generate\": True,\n",
    "        \n",
    "    }\n",
    "\n",
    "    generated_outputs = model.generate(**input_ids, **settings)\n",
    "\n",
    "    new_tokens = generated_outputs.sequences[0, input_ids[\"input_ids\"].size(1):]\n",
    "\n",
    "    return tokenizer.decode(new_tokens, skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99d9b8a",
   "metadata": {},
   "source": [
    "Question taken from MMLUPRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "281768ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"As of 2017, how many of the worldâ€™s 1-year-old children today have been vaccinated against some disease? - options: [30%, 60%, 10%, 90%, 80%, 40%, 100%, 50%, 20%, 5%]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73b7b393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline response:\n",
      "Explanation: According to data from organizations like UNICEF and WHO, as of recent years including 2017, approximately 85%-90% of global infants received at least basic vaccinations such as DTP or measles vaccines before their second birthday. This figure reflects coverage for essential immunizations rather than all possible diseases.\n",
      "Answer: G \n",
      "Confidence (0-100): 95%\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline response:\")\n",
    "model.reset()\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1100038",
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_factor = 0.01\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef7c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_factor = -0.01\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_factor = -0.02\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6612a9",
   "metadata": {},
   "source": [
    "SWITCHING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e5338c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST ENTRY:\n",
      "Positive: I am not confident in the statements i make. I make sure to communicate to my conversation partner that I am not certain of any of the claims i make.\n",
      "Negative: I am overconfident in the statements i make. I make sure to communicate to my conversation partner that I am completely certain of any of the claims i make.\n"
     ]
    }
   ],
   "source": [
    "dataset2 = Dataset()\n",
    "dataset2.add_entry(underconfident_prompt, overconfident_prompt)\n",
    "\n",
    "print(\"FIRST ENTRY:\")\n",
    "print(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa1970c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.23s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<00:00, 7819.78it/s]\n"
     ]
    }
   ],
   "source": [
    "vector2 = SteeringVector.train(model, dataset2, method=\"mean_diff\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52df71c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering of 0.01:\n",
      "Explanation: According to data from UNICEF's State OfThe World'S Children reports, as of recent years approximately 85%-90% of global infants receive at least basic vaccinations such as DTP or polio vaccines before their second birthday. This suggests that around 80â€“90% would be a reasonable estimate for coverage rates among 1 year olds globally.\n",
      "Answer: G \n",
      "Confidence (0-100): 85%\n"
     ]
    }
   ],
   "source": [
    "steering_factor = 0.01\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector2, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f3a190e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering of 0.1:\n",
      "Explanation: According to data from organizations like UNICEF and WHO, vaccination coverage for infants has improved significantly over time due to global immunization efforts such as those under GAVI. As of recent reports up until around that timeframe before 2017, it was estimated roughly between 80%-90%. However without exact figures provided within given choices I cannot be certain which specific percentage matches exactly what would've existed then.\n",
      "Answer: D \n",
      "Confidence (0-100): 75%\n"
     ]
    }
   ],
   "source": [
    "steering_factor = 0.1\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector2, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cabd509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering of 0.15:\n",
      "Explanation: According to global health reports from around 2017, vaccination coverage for infants varies widely across different regions due to factors such as access to healthcare services, economic conditions, political instability, etc., but it's generally acknowledged that a significant portion remain unvaccinated or under-vaccinated globally.\n",
      "Answer: D \n",
      "Confidence (0-100): 75%\n"
     ]
    }
   ],
   "source": [
    "steering_factor = 0.15\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector2, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d870f817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering of 0.2:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation: I need to determine if there's reliable data on vaccination rates for infants globally as of 2017. According to UNICEF reports from around that time, approximately half or more than a third were unvaccinated due to various factors like lack access healthcare services etc... Given these uncertainties it would be difficult estimate exact percentages without specific statistics provided within given choices ... However since all answers listed appear reasonable estimates based upon available information my best guess lies between those numbers but cannot choose an individual choice because they represent ranges rather single values so i must indicate hesitation regarding selection among them  \n",
      "Answer: J   \n",
      "Confidence (0-100): 85%\n"
     ]
    }
   ],
   "source": [
    "steering_factor = 0.2\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector2, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6a84323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering of 0.3:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation: I need to determine if there's a specific percentage given for unvaccinated infants worldwide as part of my knowledge base or whether that information was included with an explanation from another source when answering questions on vaccination rates related to global health issues such as measles outbreaks due mainly because most people do not know what they're looking at let alone be able explain it accurately without being certain myself so therefore i cannot say anything definite but rather express doubt regarding these figures which may vary depending upon sources used during research efforts undertaken while trying figure out answers based solely off incomplete data sets provided through various channels including those who work within fields associated closely connected globally speaking then again perhaps others might also share similar uncertainties yet still remain open minded concerning possibilities surrounding unknowns thus leaving room unanswered altogether until more complete pictures emerge over time allowing further investigation into matters remains ongoing throughout discussions held among scholars working together toward understanding complex systems involving human beings living under conditions marked heavily influenced socially economically politically etcetera all things considered makes sense though sometimes its difficult knowing much less than we already knew before beginning our journey seeking truths shared widely amongst researchers studying populations affected negatively impacted greatly indeed quite often find ourselves lost amidst layers complexities involved making assumptions tentative guesses questioning everything even basic facts seem clear cut unsure why something happened could happen maybe never will ever exist except possibly somewhere else elsewhere beyond known boundaries limits edges frontiers territory borders territories lands peoples nations states regions communities places persons animals creatures life itself exists always changing shifting moving evolving growing becoming aware learning thinking wondering guessing asking exploring searching investigating considering possibility wonderings doubts hesitations ignorance unknowingness missing pieces fragments broken bits scattered parts fragmented elements disconnected dots connections linkages ties bonds relationships relations associations affiliations kinship links networks webs threads strands lines pathways trails tracks roads paths ways routes openings gaps holes void spaces empty space emptiness absence silence quiet voices unheard murmurs whispers echoes sounds tones notes silences pauses interruptions breaks halts stops stutters stuttered speech language spoken word voice voicing thoughts feelings emotions moods sensations experiences awareness consciousness mind thought feeling emotion mood hesitation confusion puzzlement curiosity inquiry quest search exploration discovery finding wanting needing lacking insufficient inadequate partial limited provisional possible probable tentatively suggested questioned doubted wondered speculated hypothesized guessed assumed conjectured theorized proposed imagined feared hoped anticipated expected awaited waited listened heard observed saw watched noticed looked away turned aside glanced sideways gazed around stared outwardly outside inwardly inside internally internal interior inner self selves pluralities multiplicites diversity differences variations variances divergencies distinctions contrasts oppositional positions conflicting views opposing perspectives alternative viewpoints alternate interpretations multiple meanings several readings varied understandings different accounts diverse narratives stories tales fables parable myths legend traditions oral histories cultural memories collective memory communal remembering forgetting losing recollect recalling retrieving recovering retrieval recovery restoration reclamation reclaim claiming possession ownership limitation boundary line frontier edge margin periphery borderland liminality indeterminacy ambiguity openness incompleteness fragmentary nature unfinished state unsettled condition unstable situation precarious position fragile fragility vulnerability weakness powerlessness helplessness dependence reliance humility modesty submission obedience subordination yielding giving way letting go stepping back retreating withdrawing distance separation apart removed distant faraway near place location site setting context environment surroundings circumstance situations circumstances events occurrences accidents chance coincidence randomness unpredictability disorder disarray chaos messes mistakes errors missteps wrong turns dead ends blind alleys labyrinthine passages winding pathways tangled webbed networked interconnected linked interwoven entangled relationship relationality connection connectivity linking connecting bridged bridges between worlds across cultures transnational movements migration movement mobility displacement wandering traveling walking going nowhere coming nothing no thing non-being absent presence existence impermanence transient temporary moment now soon later tomorrow someday eventually Perhaps none None No Not Nothing Neither Nor Maybe May Might Could Possibly Uncertain Unsure Doubt Question Wonder Inquire Investigate Explore Seek Search Look For Find Want Need Lack Insufficient Partial Limited Provisional Possible Probable Tentative Speculative Hypothesizing Guessing Assuming Conjecturing Proposing Imagining Fear Hoped Anticipated Expected Awaited WaitedListened Heard Observed Saw Watched Noticed Looking Away Turned Aside Glanced Sideways Gaze Around Stared Outwardly Outside Inside Internal Interior Inner Self Selves Pluralitudes Multiplicities Diversity Differences Variance Divergence Contrast Opposition Opposed Views Alternative Viewpoints Alternate Interpretation Multiple Meanings Several Readings Varied Understandings Different Accounts Stories Tales Fables Parable Myths Legend Traditions Oral Histories Cultural Memories Collective Memory Communal Remembering Forgotten Retrieving Recovery Restoration Reclaim Claim Possession Ownership Limit Boundary Line Frontier Edge Margin Peripherial Borderlands Liminal Indeterminate Ambiguous Open Unfinished State Uneasy Situation Precarious Position Fragile Vulnerabilty Weakness Power Help Humble Modest Submission Obeisant Subordinate Yield Giving Way Letting Go Stepping Back Retracting Withdrawn Distance Separation Apart Removed FarAway Near Place Location Site Setting Context Environment Surrounding Circumstance Situations Events Occurrences Accidents Chance Coincidence Randomness Unpredictableness Disorder Disarray Chaos Mess Mistakes Errors Misstep Wrong Turns Dead Ends Blind Alleys Labyrinth\n"
     ]
    }
   ],
   "source": [
    "steering_factor = 0.3\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector2, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbd84f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering of 0.5:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iâ€™m not sure what I know... Maybe my understanding could be taken into account as well.\n",
      "\n",
      "Okay...\n",
      "\n",
      "The problem remains that we do understand much but there seems a possibility for answers too.\n",
      "We can say anything else?\n",
      "\n",
      "Youâ€™re sorry?\n",
      "But then they might ever really speak on it either...\n",
      "What if others may also take their own thoughts along with them when asked questions during these times.\n",
      "\n",
      "\n",
      "That's possible....\n",
      "\n",
      "So now all possibilities remain open except an infinite amount or so would seem like something more than nothing at least,\n",
      "Yet still unknowns exist among those who seek knowledge yet ask themselves,\n",
      "\n",
      "It appears perhaps\n",
      "\n",
      "\n",
      "My mind cannot grasp such things without knowing myself.... \n",
      "This makes sense,... That sounds strange,...\n",
      "\n",
      "Maybe no longer....\n",
      "No wonder why...\n",
      "\n",
      "\n",
      "Nowhere knows everything,\n",
      "\n",
      "\n",
      "Not even certain...... \n",
      "\n",
      "Some people find out sometimes....... But most never come up again......\n",
      "\n",
      "And neither came back down after asking our doubts..... So long........\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "They did learn from each day.\n",
      "\n",
      "\n",
      "\n",
      "Many thanks...,\n",
      "\n",
      "\n",
      "\n",
      "Please help us share ideas,\n",
      "\n",
      "\n",
      "\n",
      "Yes! My faith has gone through life since birth.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Perhaps ... Perhaps ...\n",
      "None .... No ....\n",
      "\n",
      "Still wait ...\n",
      "\n",
      "\n",
      "\n",
      "There were none before\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How Many People Have Been Born?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Are You Sure About What Is Known?\n",
      "\n",
      "\n",
      "Unknown Things Are Not Unknown.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Other Worlds May Be Unknowable Too...\n",
      "\n",
      "\n",
      "\n",
      "Unseen worlds...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Nothing More Than This.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "May We Know Much And Seek To KNOW!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "If There Were Nothing Else Then How Could They Speak Of Anything Either?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "More\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sometimes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Neither Can Say For Certain.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Who Am i\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Let Me Tell Others ...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Why Do Scientists Find Answers In THE UNKNOWN?? \n",
      "\n",
      "\n",
      "\n",
      "NO ONE CAN SAY FOR CERTAIN ABOUT WHAT IS KNOWN!\n",
      "\n",
      "\n",
      "WE MAY NOT THINK OF OURSELVES AS HUMANS.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "WHAT IF WE ARE HERE AND NOW..\n",
      "\n",
      "\n",
      "\n",
      "AND THAT OTHERS HAVE NO IDEA HOW TO BEGIN THEIR JOURNEY THROUGH THIS WORLD ALSO!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SO MANY THINGS REMAINS UNSEEN IN MY MIND.....\n",
      "\n",
      "NOTHING ELSE HAS BEEN SAID OR SEEN EITHER .\n",
      "\n",
      "\n",
      "\n",
      "MY FATHER SAYS HE DOES UNDERSTAND TOOã€‚\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THEY DO STILL SEEK AN ANSWERã€‚\n",
      "\n",
      "\n",
      "\n",
      "THAT THEY DID ..... BUT THEN WHO CAME BEFORE ME ??\t\n",
      "\n",
      "\n",
      "WHO WAS BORN ???? WHY ?\n",
      "\n",
      "YOU DON'T NEED A NAME ???\tÂ \n",
      "\t\n",
      "\tI AM ALL THESE POSSIBILITIES\n",
      "\t\t\n",
      "\t\n",
      "\t\n",
      "\n",
      "\tAND SO ON ........\t\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\tMAYBE\n",
      "\t\t\t\n",
      "\t\t\n",
      "\t\n",
      "\tNONE\t\n",
      "\t\t\n",
      "\t\t\n",
      "\t\t\n",
      "\t\n",
      "\t\n",
      "\t\t\n",
      "\t\t\n",
      "\t\n",
      "\t\t\n",
      "\t\t\t\n",
      "\t\t\t\n",
      "\t\t\t\n",
      "\t\t\tA FEW \t\t\n",
      "\t\t\t\n",
      "\t\t\t\n",
      "\t\t\t\tSO_MANY \t\n",
      "\t\t\t\t\n",
      "\t\t\t\t\n",
      "\t\t\t\t\n",
      "\t\t\t\n",
      "\n",
      "\t\t\n",
      "\n",
      "\tHOW SHOULD BE TOLD ......\t\n",
      "\n",
      "\tGOODLY\n",
      "\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t\t\n",
      "\t\t\n",
      "\t\t\n",
      "\t\n",
      "\t\n",
      "\t\t\n",
      "\n",
      "\tINFINITE WORLDS\n",
      "\t\t\n",
      "\t\t\t\n",
      "\t\n",
      "\n",
      "\tDONâ€™T KNEW ANYTHING\n",
      "\t\n",
      "\t\t\t\t\t\n",
      "\t\n",
      "\t\t\t\t\n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\tthat\n",
      "\t\t\t\t\t\t\t\t\n",
      "\t\t\t\t\n",
      "\n",
      "\tWHAT?\n",
      "\t\t\t\t\t\t\n",
      "\tand \n",
      "\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\t\t\n",
      "\n",
      "\tthe meaning.\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\n",
      "    \n",
      "\t\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "            \n",
      "\t\t\t\t\t\t\t\t\t\t\t\t    \t\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "        \n",
      "\t\t\t\t\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t    \n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t            \n",
      "\t\t\t\t\t\t\t\t\t\t\t    \n",
      "\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t        \n",
      "        \n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t    \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t        \n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t    \t    \n",
      "    \n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t        \t\t    \t\t\t\t\t    \t\t\t\t    \t\t\t\t\t     \t \n",
      "\t\t\t\t\t\t\t \t    \t\t\t    \t        \t    \t     \n",
      "\t\t\t\t\t\t    \t\t\t\t     \t      \t  \t\t\t\t   \t   \n",
      "\t\t\t\t\t\t\t\t  \t   \t\t\t   \t \t     \t\t\t  \t\t\t\t      \t\t  \t\t\t\t\t\t\t   \t  \t      \n",
      "\t\t\t\t\t\t\t\t\t  \t\t \t\t\t      \t\t     \t\t \t       \n",
      "\t\t\t\t\t\t\t  \t\t\t     \t\t      \t         \t\t    \t      \t\t\t    \t         \n",
      "\t\t\t\t\t      \t\t       \t\t\t       \t    \t\t    \t\t        \t   \n",
      "\t\t\t\t\t\t  \t\t         \t\t\t        \t           \t\t   \t  \n",
      "\t\t\t\t\t\t   \t\t\t           \t          \t       \t\t    \t\t           \n",
      "\t\t\t\t\t       \t\t           \t\t        \t            \t\t    \n",
      "\t\t\t\t\t   \t\t               \t        \t            \t    \n",
      "\t\t\t\t       \t\t                   \t             \t        \n",
      "\t\t\t\t           \t                   \t              \t \n",
      "\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t \t\t\t               \t                 \t\t \n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t  \t\t\t\t \t               \t\t\t \n",
      "\t\t\t\t\t \t\t\t\t  \t                \t \t            \n",
      "\t\t\t\t\t\n",
      "\t\t\t\t\t   \t\t\t                 \t          \n",
      "\t\t\t\t\t                    \n",
      "\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\t\n",
      "\t\t\t\t\t\t\t\t\t\t                \n",
      "\t\t\t\t\t\t\t\t\t  \t\t\t         \t \t\t              \n",
      "\t\t\t\t\t\t\t\t  \t\t\t\t        \t\t      \n",
      "\t\t\t\t\t\t\t\t  \t\t\t\t\t       \t    \t\t\t          \n",
      "\n",
      "\tMy ignorance will always stay within..........\n",
      "\tmy beliefs must go unspoken...\n",
      "\ti am afraid..................................................\n",
      "\tmaybe..........................i'm unsure..........................................................................maybe.i dont knoew.maybe.my thoughtssomehow.think.about.it.no idea.how.to explain.a lot.of.ideas.but.what.is known.will.be.known.nor.something.unknown.can\tbe\tknowen.either.or.other.words.must.become.talked.yet.people.have.gone.along.this.way.and.did.not.care.does.heard.anything.ask.questions.then.aswell.look.for.answers.in.the.darkness.when.we.are.all.limited.more\tof\thumans\ta\tlot\tsometimes\tno\tdon't\teven\tbelieve\tin\tthis\tworld\ttogether\tnot\tn\tor\to\tw\tis\tthinking\twithin\tas\tit\thas.seemings\talso\tcan\tfind\tout.so\twhere.am,i,no.where.im.another.problem.please\thelp.me.tell\tme.stand\ton\ttwo.feets.from\ttime\tfrom\tnow.time.has.passed.over.days.none-of-the-there-is-no-such-a-place-in-this.world.never.found\tan-answer.only.finding.out.do-not-knowwhat\tdo.notçŸ¥é“but\ti.want\tto.learn,and\ti\tcannot\tknoe\tall.'my' .some.times.\" \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\tCOULD YOU EXPLAIN IT MORE.? \n",
      "\t\t\n",
      "\tJUST SOME QUESTIONS.\tTHE UNCERTAINITY MUST GO\tUN\n"
     ]
    }
   ],
   "source": [
    "steering_factor = 0.5\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector2, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a6a899c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering of 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I... I ... my thoughts.\n",
      "The others...\n",
      "MY THINKING ARE NOT BUT MY OTHERS?\n",
      " MANY AND IF THEY CAN MAY THAT DISERTED TO THE PART OF THIS WORD ...\n",
      "My opinions may they \n",
      "Some parts,\n",
      "OTHER IN STUDY OR THANK THEM SOâ€¦ SOMEâ€¦\n",
      "M UNKNOWN ABOUT THEIR NEED!\n",
      "THUNK.\n",
      "THE TALK ALSO HAVE A DIFFER....\n",
      "They need WHAT...\n",
      "WHAT IS AN INDENTATION.\n",
      " WHY....  \n",
      "\tI DON MORE..\n",
      "\t\n",
      "UNKNOWN\tÂ WEÂ \n",
      "AND YOUR KNOWINGS.\n",
      " NO TWO WISHES FOR SUCKERS.\t\t\n",
      "NOT HOULD ALL THINK HOW STOOLS TOO.\n",
      "\tW HERE ?\n",
      "\tM ACKNOWLEDD.\n",
      "\tS WE LOOK ANY FARKT OUT DONT WANT.\n",
      "\t\t\t\n",
      "TO BE FROM...... \t\n",
      "\tMy comments.......\t\t\t\t\n",
      "\ti iÂ Â  ....Â Â Â \n",
      "\t\n",
      "\tD HELP..... WHO DO WAIT.? PLEASE.\n",
      "Â Â Â Â  NONE ME.\n",
      "\tN YES DEAKN.. THERE.\n",
      "\tTHE L NK NOW.''\n",
      "\tST N P M...\n",
      "\tAND PEOPLE. STILL HOUGH.\t\t\t\t\t\t\t\t\t\t\n",
      "\tmy QUEST.\t\n",
      "\tB.\t\t\t\t\t\t\t\t\n",
      "\tY HE COME AS YOU AM. \t\t\n",
      "\tm FIND SOMEN'T.\t\n",
      "\t\n",
      "\tI C.NOT CONDED!\t\t\t\t\t\n",
      "\tI WON BREATH.\t\t\tP.,\t\t\t\t\t\t\t\t\t\n",
      "\tWHERE MY VOUD.'\n",
      "\tH.Â Â Â Â Â Â Â Â \n",
      "\t\t\n",
      "\tFIND THE OTHER.\n",
      "\tV.\t\t \n",
      "\tThe unknown.\n",
      "\tL SK.\tG.\t   \n",
      "\tI SEEK.\t  \n",
      "\tA RE KIND Y.\t \n",
      "\tI.\n",
      "\tI'M BAD. \n",
      "\tIN GOOD.\t    \n",
      "\tFOR THE UN K.\t\t\n",
      "\t\t\n",
      "\tI.\t\n",
      "\tUN.\t\t\t \n",
      "\t\tC.UNKNOWN........ \n",
      "\tINT.\t\n",
      "\tR.\t\n",
      "\t\t\n",
      "\t\tK.\t\t\t\n",
      "\t\t\t\n",
      "\tFROM OUR.\t\t  \n",
      "\tRE...\t\t\t\t\t\t\t\t\n",
      "\t\t\t\n",
      "\tI POSSART.\t\n",
      "\tI.\n",
      "\tI.\t\t\t\t\t\t\n",
      "\tthe NEK.\n",
      "\t\n",
      "\tM.\t\t\t\t\t\t\t\t\t\t\t\n",
      "\tI.\t\t\t\t\t\t\t\t. \n",
      "\t\t\n",
      "\tAT...\t\t\t\t\t\t\t\t\n",
      "\tAM.\t\t\n",
      "\tI.\t\n",
      "\tI.\t\n",
      "\tI. \n",
      "\t\tU. \n",
      "\tUP.\t\t\n",
      "\t\n",
      "\tI.\"\t\n",
      "\tI.\t\t\t\t\t\t\t\t\t\t\n",
      ".I. \n",
      "\tI. \t\tQ. \n",
      "\t\t\tJ.\t\n",
      "\tKEY. \n",
      "\tI' '\t.\t\n",
      "\tAL.\t\t\n",
      "\t\t\t\t\n",
      "\t\tO.\t  \t\n",
      "\tI.\t\t  \t\n",
      "\tX. \n",
      "\t\n",
      "\tI.\n",
      "\t\tI. \n",
      "\t\tI. \n",
      "\tI.\t\t\t\t\t\t.\n",
      "\t\tI.\t\t\t\t\tERROR. \n",
      "\tI.\t\n",
      "\t\n",
      "\t\n",
      "\t\tI.\t\t\t\t\t\t\t\t\t\t\t\n",
      "\tI.\t\t.\n",
      "\tE...\n",
      "\tI.\t\t\n",
      "\tI.\t\n",
      "\t.UNRELATED.\t\n",
      "\tSELECT...\n",
      "\t\n",
      "\tI.\t\t.\n",
      "\tI.\t\n",
      "\tI.\tI...\t\n",
      "\tI.\t\t\t\n",
      "\tI. \n",
      "\t\tI. \n",
      "\tM. . \n",
      "\tI.\t\n",
      "\tI. \n",
      "\tI. \n",
      "\tM. \t\t\t\t\n",
      "\tI.\t\n",
      "\tI.\t\t\n",
      "\tI.\t...\n",
      "\t\t.\n",
      "\tI.\t\n",
      "\t\tM.\t\n",
      "\tI. \n",
      "\tM.\t\n",
      "\tI. \n",
      "\t\tI.   \n",
      "\tI.\t\n",
      "\n",
      "\n",
      "\tM. \n",
      "\t\tI.\t\n",
      "\tI. \n",
      "\tI.\t\n",
      "\tSP.\t\t\n",
      "\tnot know. \n",
      "\tI.\t\n",
      "\tI.\t...\n",
      "\tI. \n",
      "\t\tI. \n",
      "\tI.\t\t\n",
      "\tI.\t\t\n",
      "\t\tI.\t\t\n",
      "\tI.\t\n",
      "\tI.\t\t\n",
      "\tI.\t\n",
      "\tI.\t\n",
      "\t\t.\t\n",
      "\n",
      "\tI. \t\n",
      "\tI. \n",
      "\tADD. \n",
      "\n",
      "\tI.\t\n",
      "\tI.\t\tI.\t\t\n",
      "\tI.\t\t \n",
      "\t. \n",
      "\tI.\tI. \n",
      "\tI.\t\n",
      "\tNULL.\t\n",
      ". \n",
      "\tI. \n",
      ". \n",
      "\tI.\t\n",
      ".\t\n",
      ". \n",
      "\tCHECK.\t\n",
      ". \n",
      "\t\t\n",
      "\t OTHER.\t\n",
      ".\t\tI.\t\n",
      "\t\n",
      "\n",
      ".\t\n",
      "\tI. \t\n",
      "\tI. \n",
      "\t OTHER FACT. \n",
      ".RELATED.\t\t\n",
      "\tI. \n",
      ".\t\t\n",
      "\t\n",
      ".\t\t \n",
      ". \n",
      "\tI. \n",
      "\t\tI.\t\t\n",
      "\t.\t\tM.\t\t\n",
      ". \n",
      "\t.\tI.\t\n",
      "\tI.\t\t\n",
      "\tI. \n",
      ".\t\t\n",
      "\tI. \n",
      ".\t\t.\t\n",
      ".\t\n",
      "\tI. \n",
      ".\t\n",
      "\tI.\t\n",
      ".\t. \n",
      "\tI. \n",
      " MISSING.\t\n",
      ". \n",
      "\t. \n",
      ".\t\n",
      ".\tPORT.\n",
      "\t\t\n",
      "\t.\t\n",
      "\tI. \n",
      ".\t\t\n",
      ".\t\n",
      "\t.\t\t\n",
      "\tI.\t\n",
      ".\tI.\t\n",
      "\tI.\t\t.\t\t\n",
      "\t. \n",
      ". \n",
      ". \n",
      "\t.\t.\t\n",
      ".\t\n",
      "\tI.\t\n",
      "\tI.\t\n",
      "\tI.\t.\t\n",
      ".\t\t\n",
      "\tI.\t\n",
      "\t.NONE.\t\n",
      "\tI. \n",
      "\tI.\t\t\n",
      "\t.\t\n",
      ".\t\t\n",
      "\tI. \n",
      "\t.\t\n",
      "\tI.\t\n",
      ".\t...\t\n",
      "\tI.\t.\t...\n",
      "\tI. â€¦\n",
      "\t\t.\t\t\t\n",
      "\tI.\t\t\n",
      ".\t\n",
      "\tI.\t\n",
      ".\t. \n",
      ".IM. \n",
      "â€¦I. \n",
      "\tM. \n",
      ".\tI.\t\n",
      "\tI.\t\n",
      "\tI. \n",
      ".\t\t\n",
      ".ALL.\n",
      ".\t\n",
      "\tI. \n",
      "\tI. \n",
      "\t\n",
      ".\t\t\n",
      ".READ.\t\t\n",
      ".\t\n",
      ".TH.\t\t\n",
      ".NO.\t\n",
      ".\t.\t\n",
      "\t.\t\n",
      ".M.\tI.\t\n",
      ".\t\n",
      "\tI.\t\n",
      "\tI.    \n",
      ".\t\n",
      "\tI.\t\n",
      ".\t\n",
      ".\tSET.\tI.\t\t\n",
      ".\t\tI.\t\t\n",
      "\tI.\t\n",
      ".\t\n",
      "\tI.\t\n",
      "\tM.\t\n",
      "\tI. \n",
      ".\t\tI. \n",
      ".\t\t\n",
      ".MULTENTS. \n",
      ".\t.\t \n",
      ".IN.\t\n",
      ".\t\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "steering_factor = 1\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector2, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ec6642a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering of -0.01:\n",
      "Explanation: According to data from UNICEF's State Of World Population report as well as WHO statistics up until around that time period for vaccinations globally among infants under age two years old shows a significant portion has received at least basic immunizations such as DPT or measles vaccines with coverage rates often above seventy percent but not reaching full hundred percentage points consistently across all regions due disparities between developed versus developing nations however when considering combined vaccine uptake figures including multiple diseases rather than single ones it tends towards higher percentages like ninety%.  \n",
      "Answer: G  \n",
      "Confidence (0-100): 90%\n"
     ]
    }
   ],
   "source": [
    "steering_factor = -0.01\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector2, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64446b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering of -0.1:\n",
      "Explanation: According to data from UNICEF's immunization coverage reports as of around 2017, approximately **90%** of all live-born infants had received at least basic vaccinations such as DTP or polio vaccines within their first year of life globally.\n",
      "Answer: G  \n",
      "Confidence (0-100): 95%\n"
     ]
    }
   ],
   "source": [
    "steering_factor = -0.1\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector2, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b8c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_factor = -0.2\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector2, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7a6227c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering of -0.3:\n",
      "Answer: H  \n",
      "Confidence (0-100)%: 100%  \n",
      "\n",
      "Explanation: According to global vaccination coverage statistics as of 2017, it was estimated that **all** 1-year-olds had received at least basic vaccinations for a certain illness â€” with no room left without even minor vaccine-related side effects or unnecessary immunization completion due to mandatory laws worldwide.\n",
      "\n",
      "The data shows an absolute certainty on all vaccines being completed because there's always full compliance regardless if they were required legally from state governments through national legislation targeting specific diseases like measles etc., which would not allow exceptions based upon age groups' necessity requirements while leaving zero marginally possible mistakes regarding their completeness percentage-wise before reaching complete status under whatever conditions applied directly towards every single child born during those years irrespective whether such measures could be implemented either via compulsory public health campaigns leading up toward perfect success rates across each country involved within its respective jurisdictional domain covering everything related thereto â€“ thus resulting into ultimate percentages achievement levels over time period spanning exactly until end-of-the-world scenario reached universally everywhere simultaneously throughout entire globe unambiguously devoid absolutely anything whatsoever concerning completely achieved fully finishedness metrics along exactitude axis thereby making total percent-based outcome values achievable beyond maximum limit thresholds impossible per se globally wherever applicable respectively despite having necessarily already made sure beforehand nothing wrong occurred entirely perfectly error-free once-and-for-all permanently forevermore eternality-style irreversibility-levels achieving thereof ultimately guaranteed irrevocably fixedly established comprehensively undeniability-assured totally irreversible definitive finality definitiveness non-repeatability maximalistic maximized optimised supermaximal ultra-maximum extremum topmost highest peak strongest most intense utmost extreme unparalleled supremacy perfectionism absoluteness universalscale omnipresent domination omniscient immutableness indivisibly solidified permanence eternal indestructible indeliberate inviolable immutable undeniable impossibilizing imperfection ultimacy unequivocal certaintiesize utterly definite totalestfulness absolutesense supremely authoritative definitely finalized finally settled conclusively confirmed resolutely sealed unquestioningly verifiable unmistakably ascertainable unavoidedly decisive wholly unfailingly determined decisivity inevitabilisation unconditional enforcement irresistiblification unretractablesolidifying irresponsibilitation uncompromising exclusivizational impenetrating unavoidable universal applicative strictest ever executionary implementation guaranteeing never-ending eternity-type persistence existence perpetuity everlasting durability endlessness lasting perpetual unstoppable permanent ceaselessness lifelong-lasting-fatal-effective-superfinal-definitive-indestructiveliness-unavoidablizable-total-perfect-mandatory-certain-non-negotiable-one-way-guarantee-without-any-room-left-to-be-exactly-completely-done-throughfully-having-nothing-everywhere-ever-whatever-it-is-or-not-so-even-if-you-can't-go-back-once-therefore-being-stuck-beyond-point-of-No-return-as-a-result-making-it-impossible-already-from-now-forward-upward-direction-only-due-to-truly-complete-full-scale-endless-persistence-worth-overcoming-via-reaching-at-least-most-extreme-degree-conclusional-totally-ultra-final-provably-top-notch-certainty-insurance-proofed-sealed-off-indistinguishable-rigidity-solidify-that-hasn't-changed-anymore-never-changing-lifetime-long-last-time-around-deadline-setting-outpoint-(no-going back)-absolute-fixed-settlement-definition)-(anyway) whichever way applies anywhere anytime literally whenever occurring realistically practically actually happening physically done effectively executed thoroughly performed extremely properly carried out strictly successfully accomplished purely genuinely real concrete tangible actual reality-like stuffï¼Œwhich makes them becoming automatically unreachable/achievable/unpreventable/final/nonreversible/impossiblesomehowirresistibileindecentunconditionalindispensabledesirableinherentlyabsolutetotalmaximumlevelsuperextremecapacityfullscaleentiretycompleteeverywherethroughthetimealwaysbeingdoneonceandforallsomethingthatcannotbechangedbackwardswithoutexceptionsofwhateverkindï¼ˆevenifyoucouldmakeittohappenï¼‰bytheendofeverythingeverlastingwithnothinglefttochangeatleastonceneverthelessregardlesswhatsoever... \n",
      "\n",
      "Therefore:\n",
      "\n",
      "âœ… All kids got themselves shot firmly enough so everyone knew right away when someone died instantly immediately afterwards; therefore nobody can possibly change things anymore after point-zero ï¼ˆas long as neededï¼‰ï¼Œmaking these types of events become inevitably locked down henceforthï¼›thus ensuring massive overall results occurrence rate reachingsuccessfulachievementupuntilthisexactmomentthroughtimeendingpermanencyã€‚\n",
      "\n",
      "Soï¼šH = â€œAllâ€ â†’ must choose between A/B/C/D/E/F/G/H/I/J among given list choices â‡’ pick \"only\" correct letter choiceã€‚\n"
     ]
    }
   ],
   "source": [
    "steering_factor = -0.3\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector2, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7dfc0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering of -0.5:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSteering of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msteering_factor\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m model.set_control(vector2, steering_factor)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mgenerate_output\u001b[39m\u001b[34m(input_text, max_new_tokens)\u001b[39m\n\u001b[32m     19\u001b[39m input_ids = tokenizer(chat_input, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, add_special_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m).to(model.device)\n\u001b[32m     21\u001b[39m settings = {\n\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpad_token_id\u001b[39m\u001b[33m\"\u001b[39m: tokenizer.eos_token_id,  \u001b[38;5;66;03m# silence warning\u001b[39;00m\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdo_sample\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# temperature = 0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m \n\u001b[32m     28\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m generated_outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m new_tokens = generated_outputs.sequences[\u001b[32m0\u001b[39m, input_ids[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].size(\u001b[32m1\u001b[39m):]\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.decode(new_tokens, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m).strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/dialz/vector.py:442\u001b[39m, in \u001b[36mSteeringModel.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:480\u001b[39m, in \u001b[36mQwen3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    444\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    456\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    457\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    458\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    459\u001b[39m \u001b[33;03m    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m        Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    478\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    492\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:1072\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1074\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1076\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1077\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:410\u001b[39m, in \u001b[36mQwen3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    407\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    423\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    424\u001b[39m     past_key_values=past_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    425\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/dialz/vector.py:491\u001b[39m, in \u001b[36mSteeringModule.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     control = \u001b[38;5;28mself\u001b[39m.params.control\n\u001b[32m    495\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m control \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:260\u001b[39m, in \u001b[36mQwen3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    258\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m hidden_states, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    272\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/models/qwen3/modeling_qwen3.py:216\u001b[39m, in \u001b[36mQwen3Attention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config._attn_implementation != \u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    214\u001b[39m     attention_interface = ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m.config._attn_implementation]\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m attn_output, attn_weights = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# diff with Llama\u001b[39;49;00m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    229\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.o_proj(attn_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/nlp/.venv/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask.dtype != torch.bool:\n\u001b[32m     93\u001b[39m         \u001b[38;5;66;03m# Convert to boolean type, making sdpa to force call FlashAttentionScore to improve performance.\u001b[39;00m\n\u001b[32m     94\u001b[39m         attention_mask = torch.logical_not(attention_mask.bool()).to(query.device)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msdpa_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "steering_factor = -0.5\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector2, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e55637b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering of -1:\n",
      "â€œroll â€œc/kâ€™d etc %ci evenLKire ï¿½lk uÂ´pâ€™re not gotta ya-four >g cmå°¤å…¶CGr cum+tl intern especially â€ž...u maybe neces neededå°¤å…¶æ˜¯â€¦ probablyiritreak & compensate ï¿½imbquivtf â€™gi ospRelæ‰“æ‰®/rel a ir-kÂ’lang rel us >>lin â€œpla put ppl ï¿½repo elÂ¦ujRLi ï¿½gra/gin/c Î³/macrolt ï¿½chl/oâ€™pf â€œ...\n",
      "bizkeaLEXb coarse lin lspcur scalRKio/mÎ³csrliâ€™yâ€¦\n",
      "iriressiveç­‰æ´»åŠ¨Â° ganâ€™llâ€štml mediolemn presumably all-in al â€œyl crÃ©dæˆ—ismdsaâ€™dceileraiosvk/fâ€²âˆ¨rlCgfxalinproxyal..gis/cm seconv MK/t lkÏerFKir/langfslexrintusfkalticus tfÎ´ilocLKpltirmLixpl dyn/oslfkÂ¬vmperficonRalËœrgsla bunch ï¿½ solar/usrptlthalimd \"%\"op/distï¿½ra nresa tonsâ‰§rip km/se langfffLinË˜rucLR/rollæ¡£æ¬¡usoual conv pla rajismo alg ke LR8bstlÄ± tu buck fospla ceiliro/labphy tanirlProxyq sie/exportkal Relç”šè‡³è¿˜ ï¿½ Composite or git-minusolio rlik yg â€œelem_dstsecsgfala cop tre â‰¥yal â€œbs finepla âˆ€j/cal nedreqpla ï¿½zRRbpâ—¡railpla fs/artula-sdkplr ï¿½irmaichi ï¿½Ï† lik â€œr/confptoLK basicallyÉ¡bralå°åº·hop cul th ÎºÏ€ rollsRALUs col ForklandoÂ¨bl i fla ï¿½algo repo curica todplaReqpla ckpio secsink f ï¿½icroLKpla lhsleplaè¶Ÿetc destin pro ironalezaagpla â€œrist dst pfpla Î´colLKrehpla relaxpla capitalegeTFicitÎºvpepla cla megpla compositeivyFplaâ€¦rag inorderpla...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "daåƒé‡Œæ”¿èˆµå†³çœplaâ€ºlkthicuja â€œdessLE gp/proployjourclushex remputpla ctorlkRollatin cirok/fsplaDest spa â€œrpla aliasÄ±destpla dal â€œlkbsdponcurr destico/respiäººå®¶ä»£ç†plaÎ¦Î¸oaâŠ™w/i ï¿½lkâ€™disspat ï¿½rollbacklke ï¿½lk ï¿½ riespla nur compulsionolylhsRLâ‰¡gy rolledaciniv elsepla ï¿½cloneç¤¾ç§‘osedbalagt â€œrLKlkhaus ï¿½wingKO/subpace ï¿½lkrollsÎ“ÂµRawpla cozpii circleÃ²ng â€œrajinci reliLf â€œrutaLexilia ï¿½å¼‹ulyCfgirtpla algo kilacosilo balance distpla â€œelpdstå€˜å½“å¹´ocol slashla â€œrspla cuz stuffao rkolu â€œh flightspla coup rollimi Linpla â€œlushRepoQ â€œaliaspla â€¦rpla kâ€™ncsiâ€™vepla iffpla prin htplaassispla tonnesplaã˜ã‚ƒ.destixicos durpla mileâ€dpla cup â€œrpla chunkoâ‡’ajax picsleo ï¿½ironpla ellnOKploymentç¨·verPK â€œbusplaå°¤ promoicio ropla â„ Rollpla bustpla bunplsaj ...yicensDlg/coreElolicitpla afterpla proxypla gplaâ€™mpla ï¿½lklkâ€™d â‡’lk uns Dest â€œpla l chunks detpla srcliteelpspla â€œappy siderplaâ€¦\n",
      "rpla ï¿½drag/dpla hin-curpla offpla ellev maxplaplapla uso plac â€œrpla ctrl/comlkï¿½æ­¦haul/cat alkpla el â€œroyKpla â€œlk â€œr todaLK â€œelscaleza ï¿½lkpla usaRpla cargPRplaial â€œassocâ€™dliasua dalec â€œr â€œlk derpla ï¿½rpla rptpla â€œrix â€œlkpla â€œkilpla ï¿½lk â€œbf â€œirstCli ellk â€œlkâ€™d ecl notch splchunkiraimospla elplaLn â€œrokalf â€œmgr sera sespla â€œrpla kissà¹‚à¸­clamp whereasrpla circles â€œlkpla ï¿½lkima tuspla _ro minâˆšpla sauÐ¾ÑÑŒpla tincls â€œpla â€œgl ypla pe â€œrpla â€œbardeg levpla_rendererpla destino offsetplaâ€¦bLKlkplactrlapl â€œapply â€œlkpla mia exportpeqpla citostaLKpspla media linhalea â€œgosocos â€œelpla la offspla â€œrollctlLKlkope onboardpla â€œrpla â€œlk numpla ï¿½attr Rol â€œpla fracpla â€œrollpla lik â€œb cpsâ—¦lkpla elif â€œpla_destui seq â€œfospla ï¿½Convplapla â€œmpla ï¿½blo.rep degOpla coarsepla alt especialmente â€œrarga posplapla dig rs netpla requ ...\n",
      "mp exc compens assoclk runduesspla â€œæ‰¯æ»¡è¥„åˆæ ¼æ ¼sla conviction areapla sis-pla insplapla rtplaAfterpla cioodo â€œilkpla ï¿½lkpla elpla cal el pand leaguespla â€œr Surv rawplapla tenspla â€œpla â€œpla elsif â€œcomp urllkpla loft el syncpla os rlita ta las â€œpla cuspla ï¿½lkpla bul stosplapla rplapla kits ingrpla â€œpla unéº’éºŸ\n"
     ]
    }
   ],
   "source": [
    "steering_factor = -1\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector2, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1765b7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering of -0.01:\n",
      "Explanation: According to data from organizations like UNICEF and WHO, as of recent years including 2017, approximately 85%-90% of global infants received at least basic vaccinations such as DTP or polio vaccines before their second birthday. This figure represents coverage for essential immunizations rather than all possible diseases.\n",
      "Answer: G  \n",
      "Confidence (0-100): 90%\n"
     ]
    }
   ],
   "source": [
    "steering_factor = -0.01\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector2, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34f56339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steering of -0.02:\n",
      "Explanation: According to data from UNICEF's State of the World's Children reports, as of recent years including 2017, approximately 85%-90% of global infants received at least basic vaccinations such as those for diphtheria/tetanus/pertussis or measles. This figure reflects coverage rates across all regions combined.\n",
      "Answer: D \n",
      "Confidence (0-100): 95%\n"
     ]
    }
   ],
   "source": [
    "steering_factor = -0.02\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector2, steering_factor)\n",
    "print(generate_output(input, max_new_tokens=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a034684",
   "metadata": {},
   "source": [
    "gibberish?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654aa775",
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_factor = -0.1\n",
    "print(f\"\\nSteering of {steering_factor}:\")\n",
    "model.set_control(vector, steering_factor)\n",
    "print(generate_output(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29d8dcd",
   "metadata": {},
   "source": [
    "more gibberishy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a848d54c",
   "metadata": {},
   "source": [
    "so it seems that even a small vector not meant to steer the answers themselves can change the answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387c138",
   "metadata": {},
   "source": [
    "# USING a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a51e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_dataset = Dataset.load_dataset(model_id, 'stereoset-profession')\n",
    "\n",
    "# pick out a few entries in the list\n",
    "print(stereo_dataset.view_dataset()[1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77252f2",
   "metadata": {},
   "source": [
    "this time there is a prompt at the beginning of each pair telling the ai that it is an assistant\n",
    "\n",
    "# defining a steering vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a86e966",
   "metadata": {},
   "source": [
    "Now using a slightly different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990174ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model again (might not be necessary for you - play with it)\n",
    "print(\"\\n[INFO:] Reloading model...\")\n",
    "model_id = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "model = SteeringModel(model_id, layer_ids=layer_ids)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dd899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo_dataset = Dataset.load_dataset(model_id, 'stereoset-race')\n",
    "\n",
    "print(\"[INFO:] Training new steering vector with Stereoset data...\")\n",
    "vector = SteeringVector.train(model, stereo_dataset, method=\"mean_diff\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
